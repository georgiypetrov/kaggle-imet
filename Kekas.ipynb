{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kekas import Keker, DataOwner, DataKek\n",
    "from kekas.transformations import Transformer, to_torch, normalize\n",
    "from kekas.metrics import accuracy, bce_accuracy\n",
    "from kekas.modules import Flatten, AdaptiveConcatPool2d\n",
    "from kekas.callbacks import Callback, Callbacks, DebuggerCallback\n",
    "import pretrainedmodels as pm\n",
    "from albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n",
    "        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import islice\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "from imet.models import get_model\n",
    "from imet.dataset import TrainDataset, TTADataset, get_ids, DATA_ROOT\n",
    "from imet.transforms import train_transform, test_transform\n",
    "from imet.utils import (\n",
    "    write_event, load_model, mean_df,\n",
    "    ON_KAGGLE, set_models_path_env, seed_everything, \n",
    "    _reduce_loss, _make_mask, binarize_prediction, N_CLASSES)\n",
    "from imet.losses import loss_function\n",
    "from imet.optimizers import optimizer\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_size = 288\n",
    "fold = 0\n",
    "model = 'resnet34'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv('folds.csv')\n",
    "train_root = DATA_ROOT / 'train'\n",
    "train_fold = folds[folds['fold'] != fold]\n",
    "valid_fold = folds[folds['fold'] == fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(df: pd.DataFrame, image_transform) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            TrainDataset(train_root, df, image_transform, debug=False),\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=6,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('resnet50', num_classes=N_CLASSES, pretrained=True, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_params = list(model._classifier.parameters())\n",
    "all_params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_fn(i, row):\n",
    "    image = cv2.imread(str(train_root / f'{row[\"id\"]}.png'))[:,:,::-1]\n",
    "    image = Image.fromarray(image)\n",
    "    labels = torch.zeros(N_CLASSES)\n",
    "    for cls in row[\"attribute_ids\"].split():\n",
    "        labels[int(cls)] = 1\n",
    "    return {\"image\": image, \"labels\": labels}\n",
    "\n",
    "\n",
    "def get_transforms(dataset_key, size, p):\n",
    "    # we need to use a Transformer class to apply transformations to DataKeks elements\n",
    "    # dataset_key is an image key in dict returned by reader_fn\n",
    "    \n",
    "    AUGS = Transformer(dataset_key, train_transform(input_size))\n",
    "                           \n",
    "    TO_ARRAY = Transformer(dataset_key, lambda x: np.array(x))\n",
    "\n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    train_tfms = transforms.Compose([AUGS, TO_ARRAY, NRM_TFMS])\n",
    "    val_tfms = transforms.Compose([TO_ARRAY, NRM_TFMS])  # because we don't want to augment val set yet\n",
    "    \n",
    "    return train_tfms, val_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms, val_tfms = get_transforms(\"image\", input_size, 0.5)\n",
    "\n",
    "train_dk = DataKek(df=train_fold, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=valid_fold, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=6, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"labels\",                 # remember, we defined it in the reader_fn for DataKek?\n",
    "              metrics={\"acc\": bce_accuracy},          # optional, you can not specify any metrics at all\n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  10% 275/2731 [01:29<13:23,  3.06it/s, loss=0.0207]"
     ]
    }
   ],
   "source": [
    "keker.kek_lr(final_lr=0.1, logdir=\"logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
